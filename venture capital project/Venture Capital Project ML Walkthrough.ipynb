{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import pre-processed dataframe\n",
    "ml_df = pd.read_csv('C:/datascience/springboard/projects/Venture Capital/data/Exported Data/ML Dataframe.csv', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20621 entries, 0 to 20620\n",
      "Data columns (total 58 columns):\n",
      "company_name         20621 non-null object\n",
      "status               20621 non-null object\n",
      "funding_rounds       20621 non-null float64\n",
      "invscore_1           20621 non-null float64\n",
      "invscore_10          20621 non-null float64\n",
      "invscore_11          20621 non-null float64\n",
      "invscore_12          20621 non-null float64\n",
      "invscore_2           20621 non-null float64\n",
      "invscore_3           20621 non-null float64\n",
      "invscore_4           20621 non-null float64\n",
      "invscore_5           20621 non-null float64\n",
      "invscore_6           20621 non-null float64\n",
      "invscore_7           20621 non-null float64\n",
      "invscore_8           20621 non-null float64\n",
      "invscore_9           20621 non-null float64\n",
      "invct_1              20621 non-null float64\n",
      "invct_10             20621 non-null float64\n",
      "invct_11             20621 non-null float64\n",
      "invct_12             20621 non-null float64\n",
      "invct_2              20621 non-null float64\n",
      "invct_3              20621 non-null float64\n",
      "invct_4              20621 non-null float64\n",
      "invct_5              20621 non-null float64\n",
      "invct_6              20621 non-null float64\n",
      "invct_7              20621 non-null float64\n",
      "invct_8              20621 non-null float64\n",
      "invct_9              20621 non-null float64\n",
      "rddt_diff_1          20621 non-null float64\n",
      "rddt_diff_2          20621 non-null float64\n",
      "rddt_diff_3          20621 non-null float64\n",
      "rddt_diff_4          20621 non-null float64\n",
      "rddt_diff_5          20621 non-null float64\n",
      "rddt_diff_6          20621 non-null float64\n",
      "rddt_diff_7          20621 non-null float64\n",
      "rddt_diff_8          20621 non-null float64\n",
      "rddt_diff_9          20621 non-null float64\n",
      "rddt_diff_10         20621 non-null float64\n",
      "rddt_diff_11         20621 non-null float64\n",
      "rddt_diff_12         20621 non-null float64\n",
      "rddt_diff_13         20621 non-null float64\n",
      "rddt_diff_14         20621 non-null float64\n",
      "rdamt_1              20621 non-null float64\n",
      "rdamt_2              20621 non-null float64\n",
      "rdamt_3              20621 non-null float64\n",
      "rdamt_4              20621 non-null float64\n",
      "rdamt_5              20621 non-null float64\n",
      "rdamt_6              20621 non-null float64\n",
      "rdamt_7              20621 non-null float64\n",
      "rdamt_8              20621 non-null float64\n",
      "rdamt_9              20621 non-null float64\n",
      "rdamt_10             20621 non-null float64\n",
      "rdamt_11             20621 non-null float64\n",
      "rdamt_12             20621 non-null float64\n",
      "rdamt_13             20621 non-null float64\n",
      "rdamt_14             20621 non-null float64\n",
      "funding_total_usd    20621 non-null float64\n",
      "category_code        20621 non-null int64\n",
      "region_code          20621 non-null int64\n",
      "dtypes: float64(54), int64(2), object(2)\n",
      "memory usage: 9.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#Show summary information of Venture Capital Dataframe\n",
    "ml_df.info(verbose=True)\n",
    "\n",
    "#Set index to company name for reference (not a feature)\n",
    "ml_df = ml_df.set_index('company_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Dataset Target Label Counts: \n",
      " operating    15157\n",
      "acquired      4567\n",
      "closed         553\n",
      "ipo            344\n",
      "Name: status, dtype: int64 \n",
      "\n",
      "Full Dataset Target as % of Total: \n",
      " operating    0.74\n",
      "acquired     0.22\n",
      "closed       0.03\n",
      "ipo          0.02\n",
      "Name: status, dtype: float64 \n",
      "\n",
      "Target Labels ['acquired' 'closed' 'ipo' 'operating'] \n",
      " Label Numbers [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "#Before Encoding print value_counts of each label (4)\n",
    "print(\"Full Dataset Target Label Counts:\", '\\n', ml_df['status'].value_counts(), '\\n')\n",
    "print(\"Full Dataset Target as % of Total:\", '\\n', round(ml_df['status'].value_counts() / len(ml_df), 2), '\\n')\n",
    "\n",
    "#Convert df to array and perform one-hot encoding for target variable 'status'\n",
    "tgt_enc = LabelEncoder().fit(ml_df['status'])\n",
    "tgt_encoded = tgt_enc.transform(ml_df['status'])\n",
    "ml_df = ml_df.drop(columns='status')\n",
    "\n",
    "#Print Status Labels and Numbers for reference\n",
    "print(\"Target Labels\", np.unique(tgt_enc.inverse_transform(tgt_encoded)), '\\n' ,\"Label Numbers\", np.unique(tgt_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that 74% of the companies in the dataset are companies still operating, one strategy would be to just predict that every company is operating. This would result in 74% accuracy, therefore one of my criteria is to acheive an accuracy score of greater than 74% to be considered useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert dataframe to array and separate feature data (X) from target data (Y)\n",
    "X = ml_df.values\n",
    "Y = tgt_encoded\n",
    "\n",
    "#Scale feature data to acheieve mean of zero for each feature to account for large variation of values\n",
    "scaled_X = StandardScaler().fit_transform(X)\n",
    "\n",
    "#Train, test, split data using 70% of data for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, Y, test_size=0.3) #Random State?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_RandTree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b84d48bbe6b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfeatures_RandTree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimportance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mml_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_RandTree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mfeatures_RandTree\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_RandTree' is not defined"
     ]
    }
   ],
   "source": [
    "#Visualize feature importance\n",
    "features_RandTree = {}\n",
    "\n",
    "for feature, importance in zip(ml_df.columns, model_RandTree.feature_importances_):\n",
    "    features_RandTree[feature] = importance\n",
    "\n",
    "importances_RandTree = pd.DataFrame.from_dict(features_RandTree, orient='index').rename(columns={0: 'Gini-Importance'})\n",
    "importances_RandTree = importances_RandTree.sort_values(by='Gini-Importance', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,12))\n",
    "plt.barh(importances_RandTree.index, importances_RandTree['Gini-Importance'])\n",
    "plt.title('Feature Importance - Random Forest Classifier', fontsize=17)\n",
    "ax.set_ylabel('Features', fontsize=16)\n",
    "ax.set_yticklabels(importances_RandTree.index, fontsize=15)\n",
    "ax.set_xlabel('Gini-Importance', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset original df using identified highest importance features (10% threshold)\n",
    "sfm_RandTree = SelectFromModel(model_RandTree, threshold=0.10)\n",
    "sfm_RandTree.fit(X_train, y_train)\n",
    "\n",
    "#Print selected features\n",
    "for idx, feature_list_index in enumerate(sfm_RandTree.get_support(indices=True)):\n",
    "    i = idx + 1\n",
    "    print(\"Selected Feature %s:\" % i, ml_df.columns[feature_list_index])\n",
    "\n",
    "#Transform data based on feature selection\n",
    "X_important_train = sfm_RandTree.transform(X_train)\n",
    "X_important_test = sfm_RandTree.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter Tuning for n_estimators. Using selected features.\n",
    "n_estimators = range(1, 202, 5)\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for estimator in n_estimators:\n",
    "    #Training data\n",
    "    rf = RandomForestClassifier(n_estimators=estimator, n_jobs=-1)\n",
    "    rf.fit(X_important_train, y_train)\n",
    "    train_pred = rf.predict(X_important_train)\n",
    "    train_score = round(accuracy_score(y_train, train_pred),2)\n",
    "    train_results.append(train_score)\n",
    "\n",
    "    #Test Data\n",
    "    test_pred = rf.predict(X_important_test)\n",
    "    test_score = round(accuracy_score(y_test, test_pred),2)\n",
    "    test_results.append(test_score)\n",
    "\n",
    "#Plot Results\n",
    "train_line = plt.plot(n_estimators, train_results, color='blue', label = 'Train Score')\n",
    "test_line = plt.plot(n_estimators, test_results, color='red', label = 'Test Score')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter Tuning for max_depth. Using selected features and n_estimators=15.\n",
    "rf_depth = range(1, 50, 1)\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for depth in rf_depth:\n",
    "    #Training data\n",
    "    rf = RandomForestClassifier(max_depth=depth, n_estimators=50, n_jobs=-1)\n",
    "    rf.fit(X_important_train, y_train)\n",
    "    train_pred = rf.predict(X_important_train)\n",
    "    train_score = round(accuracy_score(y_train, train_pred),2)\n",
    "    train_results.append(train_score)\n",
    "\n",
    "    #Test Data\n",
    "    test_pred = rf.predict(X_important_test)\n",
    "    test_score = round(accuracy_score(y_test, test_pred),2)\n",
    "    test_results.append(test_score)\n",
    "\n",
    "#Plot Results\n",
    "train_line = plt.plot(rf_depth, train_results, color='blue', label = 'Train Score')\n",
    "test_line = plt.plot(rf_depth, test_results, color='red', label = 'Test Score')\n",
    "plt.legend()\n",
    "plt.xlabel('Max Depth of Random Forest')\n",
    "plt.ylabel('Accuracy Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter Tuning for min_samples_split and min_samples_leaf. Using selected features and n_estimators=50.\n",
    "min_sample = np.arange(0.05, 0.50, 0.05)\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for sample in min_sample:\n",
    "    #Training data\n",
    "    #I used same code to test leafs, just changed min_samples_split to min_samples_leaf (changed max step value to 0.5 as well)\n",
    "    rf = RandomForestClassifier(n_estimators=50, min_samples_leaf=sample, n_jobs=-1)\n",
    "    rf.fit(X_important_train, y_train)\n",
    "    train_pred = rf.predict(X_important_train)\n",
    "    train_score = round(accuracy_score(y_train, train_pred),2)\n",
    "    train_results.append(train_score)\n",
    "\n",
    "    #Test Data\n",
    "    test_pred = rf.predict(X_important_test)\n",
    "    test_score = round(accuracy_score(y_test, test_pred),2)\n",
    "    test_results.append(test_score)\n",
    "\n",
    "#Plot Results\n",
    "train_line = plt.plot(min_sample, train_results, color='blue', label = 'Train Score')\n",
    "test_line = plt.plot(min_sample, test_results, color='red', label = 'Test Score')\n",
    "plt.legend()\n",
    "plt.xlabel('Minimum Sample')\n",
    "plt.ylabel('Accuracy Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using n_estimators=50 and max_depth=20. I decided not to create parameters for min_samples of min_leaf as some of the classes\n",
    "#had smaller sample sets and didn't want to set thresholds that would potentially ignore these classes. \n",
    "#Instantiate RandomForestClassifier for deeper evaulation. \n",
    "model_RandTree = RandomForestClassifier(max_depth=10, n_estimators=50, random_state=0, n_jobs=-1)\n",
    "\n",
    "#Train and create prediction values for full dataset \n",
    "model_RandTree.fit(X_train, y_train)\n",
    "y_pred_RandTree = model_RandTree.predict(X_test)\n",
    "\n",
    "#Train and create prediction values for selected dataset\n",
    "model_RandTree.fit(X_important_train, y_train)\n",
    "y_pred_RandTree_important = model_RandTree.predict(X_important_test)\n",
    "\n",
    "#Verify model inputs\n",
    "model_RandTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph confusion matrix for full dataset and dataset using top features\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,15))\n",
    "mat1 = confusion_matrix(y_test, y_pred_RandTree)\n",
    "mat2 = confusion_matrix(y_test, y_pred_RandTree_important)\n",
    "\n",
    "sns.heatmap(mat1.T, square=True, annot=True, fmt='d', cbar=False, ax=ax1)\n",
    "ax1.set_title('Confusion Matrix - Full Features (accuracy = %s percent)' % \n",
    "              round(accuracy_score(y_test, y_pred_RandTree),2), fontsize=15)\n",
    "ax1.set_xlabel('True Label', fontsize=13)\n",
    "ax1.set_ylabel('Predicted Label', fontsize=13)\n",
    "bottom, top = ax1.get_ylim()\n",
    "ax1.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "sns.heatmap(mat2.T, square=True, annot=True, fmt='d', cbar=False, ax=ax2)\n",
    "ax2.set_title('Confusion Matrix - Important Features (accuracy = %s percent)' % \n",
    "              round(accuracy_score(y_test, y_pred_RandTree_important),2), fontsize=15)\n",
    "ax2.set_xlabel('True Label', fontsize=13)\n",
    "ax2.set_ylabel('Predicted Label', fontsize=13)\n",
    "bottom, top = ax2.get_ylim()\n",
    "ax2.set_ylim(bottom + 0.5, top- 0.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Print Status Labels and Numbers for reference\n",
    "print(\"Target Labels\", np.unique(tgt_enc.inverse_transform(tgt_encoded)), '\\n' ,\"Label Numbers\", np.unique(tgt_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Report\n",
    "print(\"Random Forest - Full Dataset:\" + '\\n' + classification_report(y_test, y_pred_RandTree, target_names=np.unique(tgt_enc.inverse_transform(tgt_encoded))))\n",
    "print(\"Random Forest - Important Dataset:\" + '\\n' + classification_report(y_test, y_pred_RandTree_important, target_names=np.unique(tgt_enc.inverse_transform(tgt_encoded))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Tree Plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "1) The Random Forest Model predicted approximately 62% of the acquired companies correctly (recall). Considering acquisitions comprised 22% of the original dataset, I consider this accuracy to be reasonably good.\n",
    "\n",
    "2) Predicted results were given multi-class labels, but predictions were mostly binary. We likely don't have enough datapoints on companies that did an Initial Public Offering (IPO) or Closed. The data distinguished between operating and acquired, which was 96% of the original dataset. This would argue that the model would need to predict at least 50% of the acquisitions to be useful (i.e. binary classification).\n",
    "\n",
    "3) The average weighted F1 score was 84%, which is useful for class imbalanced data such as this. \n",
    "\n",
    "4) The results were virtually identical regardless of if you used the three features or the full dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
